{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"주택 가격 예측:회귀 문제.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM29tyk0dDIqCOeqAkHxYsR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"h2x4Ajh3chx7"},"source":["import keras\n","keras.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mxigrwwhQu7U"},"source":["### 1970년 중반 보스턴 외곽 지역의 범죄율, 지방세율 등의 데이터가 주어졌을 때 주택 가격의 중간 값을 예측"]},{"cell_type":"code","metadata":{"id":"DbDs8OQ_cqgD"},"source":["from keras.datasets import boston_housing\n","\n","(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uQw6I0amQNgJ"},"source":["# 데이터 포인트가 506개로 비교적 개수가 적고 404개는 훈련 샘플, 102개는 테스트 샘플로 나뉨\n","print(train_data.shape)\n","print(test_data.shape)\n","# 입력 데이터에 있는 각 특성은 스케일이 서로 다름 (ex 범죄율)\n","# 어떤 값은 0 과 1 사이의 비율을 나타내고, 어떤 것은 1 과 12 사이의 값을 가지거나 1 과 100 사이의 값을 가짐"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FtMPKQE3Rc1F"},"source":["# 13개 데이터의 특성\n","1. Per capita crime rate.\n","1. Proportion of residential land zoned for lots over 25,000 square feet.\n","1. Proportion of non-retail business acres per town.\n","1. Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n","1. Nitric oxides concentration (parts per 10 million).\n","1. Average number of rooms per dwelling.\n","1. Proportion of owner-occupied units built prior to 1940.\n","1. Weighted distances to five Boston employment centres.\n","1. Index of accessibility to radial highways.\n","1. Full-value property-tax rate per $10,000.\n","1. Pupil-teacher ratio by town.\n","1. 1000 * (Bk - 0.63) ** 2 where Bk is the proportion of Black people by town.\n","1. % lower status of the population."]},{"cell_type":"code","metadata":{"id":"AHNDHYe_Qqb3"},"source":["train_targets # 타겟은 주택의 중간 가격으로 (단위:1000달러)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DTbOxTGDRrso"},"source":["mean = train_data.mean(axis=0)\n","train_data -= mean\n","std = train_data.std(axis=0)\n","train_data /= std\n","# 상이한 스케일을 가진 값은 신경망에 문제가 됨\n","# 다양한 데이터에 자동으로 맞추려고 할 수 있겠지만 학습을 더 어렵게 만들기 때문에\n","# 이런 데이터를 다룰 땐 대표적인 방법으로 정규화를 함\n","\n","\n","# 각 특성에 대해 특성의 평균을 뺀 뒤, 표준 편차로 나눔\n","# 특성의 중앙이 0 근처에 맞춰지고 표준 편차가 1이 됨\n","test_data -= mean\n","test_data /= std"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MjFUq1sSSwjn"},"source":["####### 이때 주목해야 할 점\n","####### 테스트 데이터를 정규화 할 때 사용한 값이 훈련 데이터에서 계산한 값 임을 주목\n","####### 머신 러닝 작업 과정에서 절대로 테스트 데이터에서 계산한 어떤 값도 사용해서는 안 됨 // 데이터 정규화처럼 간단한 작업조차도 하면 안됨"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rlDKh5nWTOs_"},"source":["from keras import models\n","from keras import layers\n","import tensorflow as tf\n","\n","def build_model():\n","  # 동일한 모델을 여러번 생성할 것이므로 함수를 만들어 사용\n","\n","  model = models.Sequential()\n","  model.add(layers.Dense(64, activation='relu',\n","                         input_shape=(train_data.shape[1],)))\n","  model.add(layers.Dense(64, activation='relu'))\n","  model.add(layers.Dense(1))\n","  # 이 네트워크의 마지막 층은 하나의 유닛을 가지고 있고 활성화 함수가 없음 (선형 층 이라고 함)\n","  # 전형적인 스칼라 회귀 (하나의 연석적인 값을 예측하는 회귀) 를 위한 고성\n","  # \"mes = mean_squared_error\" 의 약자\n","\n","  # model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n","  # KeyError: 'val_mean_absolute_error' 해결법\n","  # 이렇게 작성 된 것을, 아래처럼 바꾸라는 것\n","  model.compile(optimizer='rmsprop', loss='mse', metrics=[tf.keras.metrics.MeanAbsoluteError()])\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AtGqWcwKTsG4"},"source":["# K-겹 검증을 사용한 훈련 검증\n","# (훈련에 사용할 에포크의 수 같은) 매개변수들을 조정하면서 모델을 평가하기 위해 이전 예제에서 했던 것처럼 데이터를 훈련 세트와 검증 세트로 나눕니다.\n","# 데이터 포인트가 많지 않기 때문에 검증 세트도 매우 작아집니다(약 100개의 샘플). 결국 검증 세트와 훈련 세트로 어떤 데이터 포인트가 선택됐는지에 따라 검증 점수가 크게 달라집니다.\n","# 검증 세트의 분할에 대한 검증 점수의 분산이 높습니다. 이렇게 되면 신뢰있는 모델 평가를 신뢰있게 할 수 없습니다.\n","# 이런 상황에서 가장 좋은 방법은 K-겹 교차 검증을 사용하는 것입니다.\n","# 데이터를 K개의 분할(즉, 폴드)로 나누고(일반적으로 K = 4 또는 5), K개의 모델을 각각 만들어 K - 1개의 분할에서 훈련하고 나머지 분할에서 평가하는 방법입니다.\n","# 모델의 검증 점수는 K 개의 검증 점수의 평균이 됩니다."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fPuDWMmhUuav"},"source":["import numpy as np\n","\n","k = 4\n","num_val_samples = len(train_data) // k\n","num_epochs = 100\n","all_scores = []\n","\n","for i in range(k):\n","  print(\"처리중인 폴드 #\", i)\n","  # 검증 데이터 준비 : k번째 분할\n","  val_data = train_data[i * num_val_samples : (i + 1) * num_val_samples]\n","  val_targets = train_targets[i * num_val_samples : (i + 1) * num_val_samples]\n","\n","  # 훈련 데이터 준비 : 다른 분할 전체\n","  partial_train_data = np.concatenate(\n","      [train_data[ : i * num_val_samples],\n","       train_data[(i + 1) * num_val_samples : ]],\n","       axis = 0)\n","  partial_train_targets = np.concatenate(\n","      [train_targets[ : i * num_val_samples],\n","       train_targets[(i + 1) * num_val_samples : ]],\n","       axis = 0)\n","  \n","  # keras 모델 구성 (compile 포함) // 윗윗 칸에서 만듦\n","  model = build_model()\n","  \n","  # 모델 훈련(verbose=0 이므로 훈련 과정이 출력되지 않음\n","  model.fit(partial_train_data, partial_train_targets, epochs=num_epochs, batch_size=1, verbose=0)\n","  \n","  # 검증 세트로 모델 평가\n","  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n","  all_scores.append(val_mae)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zYxblOC-YsoV"},"source":["print(all_scores)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tL8WXIXLZhui"},"source":["print(np.mean(all_scores))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NuSLynyvZ1qN"},"source":["from keras import backend as K\n","\n","# 메모리 해제\n","K.clear_session()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ehiu4juqaGqd"},"source":["num_epochs = 500\n","all_mae_histories = []\n","\n","k = 4\n","num_val_samples = len(train_data) // k\n","\n","for i in range(k):\n","  print(\"처리중인 폴드 #\", i)\n","  # 검증 데이터 준비\n","  val_data = train_data[i * num_val_samples : (i + 1) * num_val_samples]\n","  val_targets = train_targets[i * num_val_samples : (i + 1) * num_val_samples]\n","\n","  # 훈련 데이터 준비\n","  partial_train_data = np.concatenate(\n","      [train_data[ : i * num_val_samples],\n","       train_data[(i + 1) * num_val_samples : ]],\n","       axis = 0)\n","  partial_train_targets = np.concatenate(\n","      [train_targets[ : i * num_val_samples],\n","       train_targets[(i + 1) * num_val_samples : ]],\n","       axis = 0)\n","  \n","  # keras 모델 \n","  # KeyError: 'val_mean_absolute_error' 해결법 // build_model() 안에 주석으로 적어 둠\n","  model = build_model() \n","  \n","  # 모델 훈련(verbose=0 이므로 훈련 과정이 출력되지 않음\n","  history = model.fit(partial_train_data, partial_train_targets, validation_data=(val_data, val_targets), epochs=num_epochs, batch_size=1, verbose=0)\n","  \n","  # 검증 세트로 모델 평가\n","  mae_history = history.history['val_mean_absolute_error']\n","                                #val_mean_absolute_error\n","  all_mae_histories.append(mae_history)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ydTq3J0Reqhd"},"source":["# num_epochs = 500\n","# all_mae_histories = []\n","# for i in range(k):\n","#     print('처리중인 폴드 #', i)\n","#     # 검증 데이터 준비: k번째 분할\n","#     val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n","#     val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n","\n","#     # 훈련 데이터 준비: 다른 분할 전체\n","#     partial_train_data = np.concatenate(\n","#         [train_data[:i * num_val_samples],\n","#          train_data[(i + 1) * num_val_samples:]],\n","#         axis=0)\n","#     partial_train_targets = np.concatenate(\n","#         [train_targets[:i * num_val_samples],\n","#          train_targets[(i + 1) * num_val_samples:]],\n","#         axis=0)\n","\n","#     # 케라스 모델 구성(컴파일 포함)\n","#     model = build_model()\n","#     # 모델 훈련(verbose=0 이므로 훈련 과정이 출력되지 않습니다)\n","#     history = model.fit(partial_train_data, partial_train_targets,\n","#                         validation_data=(val_data, val_targets),\n","#                         epochs=num_epochs, batch_size=1, verbose=0)\n","#     mae_history = history.history['val_mean_absolute_error']\n","#     all_mae_histories.append(mae_history)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k9vKQMyPbem0"},"source":["average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mSI1wuF3c1MR"},"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n","plt.xlabel('Epochs')\n","plt.ylabel('Validation MAE')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Smw75mnnc1HS"},"source":["def smooth_curve(points, factor=0.9):\n","  smoothed_points = []\n","  \n","  for point in points:\n","    if smoothed_points:\n","      previous = smoothed_points[-1]\n","      smoothed_points.append(previous * factor + point * (1 - factor))\n","    else:\n","      smoothed_points.append(point)\n","  return smoothed_points\n","\n","smooth_mae_history = smooth_curve(average_mae_history[10:])\n","\n","plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\n","plt.xlabel('Epochs')\n","plt.ylabel('Validation MAE')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_owVTT8Fc0_B"},"source":["# 새롭게 컴파일 된 모델\n","model = build_model()\n","\n","# 전체 데이터로 훈련\n","model.fit(train_data, train_targets, epochs=80, batch_size=16, verbose=0)\n","\n","test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sbU54vf7f6fZ"},"source":["test_mae_score"],"execution_count":null,"outputs":[]}]}